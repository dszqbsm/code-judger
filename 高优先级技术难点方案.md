# 高并发设计

1. 前端层：基于JavaScript监听“提交”按钮点击事件，限制提交频率

2. 接入层：在k8s集群外部署nginx作为公网入口，nginx负载均衡选择k8s集群的提交服务service（**高**）；k8s内置kube-proxy进行负载均衡，将请求分发给提交服务pod进行处理（**高**）

3. 服务层：前端限制提交频率易被绕过，需要加入redis用户级限流、全局限流兜底策略；基于ants库创建大小固定的协程池，所有请求的处理逻辑都交给协程池执行，请求量超过协程池容量则放入带缓冲的channel（任务队列）中，队列满则返回错误（**高**）

4. 任务缓冲层：kafka队列设计，削峰填谷，解耦提交服务和判题服务，按“题目ID哈希”分区（确保同一题目的提交任务分配到同一分区），多少个判题服务对应多少个分区（每个实例消费一个分区，避免任务重复处理）（**高**）


不应该直接把实现限制在某一个问题中，应该把整个流程的全部实现梳理出来，因为什么要这样做，这样做引入了什么问题又是怎么解决，最后再根据具体的问题，从实现中拿出来做回答

1. 为了防止用户恶意刷题，在前端层面基于JavaScript监听“提交”按钮点击事件，限制提交频率，而前端容易被绕过，在服务层提供分布式限流机制作为兜底，实现用户级限流与全局限流，通过redis缓存，创建用户提交次数记录和全局提交次数记录，当超过限定阈值时拒绝用户的提交请求

2. 为了避免单点过载问题，提高系统的负载能力，设计了双层负载均衡策略，通过nginx作为边缘层对后端的提交服务service做负载均衡，请求来到提交服务service后，通过k8s再对后端的提交服务pod做负载均衡

3. 为了避免无限制创建goroutine，采用ants库实现协程池，请求优先调用协程池中的协程进行处理，超过协程池容量则放入一个带缓冲的channel中，队列满则返回错误

4. 为了避免判题服务被瞬时流量压垮，引入消息队列削峰填谷，异步处理判题任务，按“题目ID哈希”分区确保同一题目的提交任务分配到同一分区，每个判题服务实例消费一个分区，避免任务重复处理

5. 为了提升用户体验，提交任务校验通过后立即生成一个提交id并返回用户，采用批量异步的方式写入数据库，减少数据库io次数，并将用户状态、题目状态等热点数据存入缓存，减少rpc调用次数


最最有优先的是底层判题任务如何执行

1. **代码安全问题，防止用户代码攻击系统**

1.1. **采用chroot实现*文件隔离***
原理是修改进程的根目录指针，从而限制进程只能访问指定目录内的文件，底层原理是Linux内核中，每个进程的进程描述符都包含一个fs字段指向fs_struct结构体，其中root成员是一个指向进程当前的根目录的指针，当调用sys_chroot时，内核会解析传入的目录路径，验证调用者是否有该目录的执行权限，再将root指针更新为该目录，后续该进程的所有路径解析都会以新的root作为起点，如调用sys_chroot指令传入目录/judger/run/123，后续进程访问/etc/passwd实际访问的是/judger/run/123/etc/passwd，若这个文件不存在则会访问失败，从而实现了文件隔离
在判题系统实现中，我们仅复制运行程序必须的文件到chroot目录中，从而最小化判题文件环境，此外还通过权限控制防止chroot逃逸（即若进程有root权限且能chdir（与cd命令功能相同，用于改变当前所在的工作目录）到上层目录，则可再次chroot突破）的发生
以上实现能阻止用户代码访问敏感文件，从文件系统层面切断攻击路径

1.2. **采用Linux Namespaces实现*资源隔离***
原理是通过为进程创建独立的资源视图，Linux Namespaces是Linux内核提供的一种全局资源隔离机制，其核心是通过为进程创建独立的资源视图，使得不同Namespaces中的进程仿佛运行在完全独立的系统环境中，无法感知到其他Namespace的资源，这种隔离也是容器技术Docker、Kubernetes的底层基础。Linux内核中每个进程的进程描述符都有一个nsproxy指针指向该进程所属的所有Namespace，并且新进程默认继承父进程的Namespace，但是父进程可以在创建子进程时将子进程的nsproxy指向全新的Namespace集合，当进程访问全局资源时，内核会先检查进程所属的Namespace，返回该Namespace内的资源视图，而不是全局资源

Linux七种Namespaces对应的全局资源
创建新Namespace时创建新资源，并由内核维护映射关系：
 - *PID Namespace*：管理进程id、进程组id、会话id，不同Namespace中的进程id独立，且看不到其他Namespace中的进程；内核维护“进程id映射表”，所有进程默认共享这个pid编号，通过创建新的PID Namespace可以创建独立的PID映射表，此外内核会维护不同Namespace间的pid映射关系；判题系统中可以用来防止用户代码通过kill终止主机进程，或通过ps窥探主机进程列表
 - *Cgroup Namespace*：管理控制组的视图和权限，每个Namespace仅能看到自身关联的cgroup子树；内核的cgroup文件系统是全局资源，默认进程可以看到所有cgroup子树，创建新Cgroup Namespace时，内核会对其Cgroup映射到主机Cgroup的某个子目录，如新cgroup Namespace的cgroup（/sys/fs/cgroup/）其实对应主机的/sys/fs/cgroup/judge/123，无法访问主机其他cgroup目录；判题系统中用来防止用户代码修改cgroup配置

创建新Namespace时复制父Namespace对应的资源，新Namespace中修改不影响父Namespace资源
 - *Mount Namespace*：管理文件系统挂载点，每个Namespace中文件系统挂载操作仅对自身Namespace的挂载点生效，不影响其他主机或Namespace的挂载点；内核挂载点列表是全局资源，默认所有进程看到的文件系统结构都是一样的，通过Mount Namespace创建新的Namespace时，会复制当前Namespace的挂载点列表，但是Namespace内的操作不会影响父进程；判题系统中可以用来防止用户通过mount挂在主机敏感分区，或umount破坏主机文件系统
 - *User Namespace*：管理用户id、组id、权限能力，每个Namespace有独立的uid/gid映射；内核的uid/gid权限是全局资源，默认进程的uid/gid直接对应主机的用户身份，创建新User Namespace可以进行uid/gid映射修改，即将沙箱内的root权限uid映射到主机中普通用户的uid，从而沙箱中无法执行主机级别的特权操作；判题系统中可以用来限制权限滥用，即获取沙箱中的root也无法获取主机的真实特权
 - *UTS Namespace*：管理主机名、域名，每个Namespace可以设置独立的主机名/域名，进程获取的主机信息仅为所属Namespace的配置；内核的系统标识是全局资源，默认所有进程获取的都是主机的真实主机名，创建新UTS Namespace会复制当前Namespace的主机名/域名作为初始值，后续修改仅对自身生效；判题系统中可以用来避免用户代码修改主机名干扰系统标识，或通过主机名判断系统环境

创建新Namespace时创建独立新资源
 - *Network Namespace*：管理网络设备如网卡、lo，IP地址、端口、路由表、防火墙规则、套接字，每个Namespace都有独立的网络栈，进程无法访问其他Namespace的网络资源；内核维护的网络子系统是全局资源，所有进程默认共享主机的网络栈，但可以通过Newtork Namespace为新Namespace创建一套最小化网络栈；判题系统中可以用来禁止代码的网络访问，防止恶意代码发起网络攻击或泄漏主机信息
 - *IPC Namespace*：管理进程间通信资源，如消息队列、共享内存、信号量，每个Namespace内的IPC资源仅对自身可见，进程无法通过IPC与其他Namespace的进程通信；内核的IPC资源池是全局资源，默认不同进程可以通过相同的IPC访问同一资源，创建新IPC Namespace时会分配独立的IPC资源池，仅在当前Namespace中可见；判题系统中可以用来防止用户代码通过共享内存读取主机进程的敏感数据，或通过消息队列与其他进程通信

文件系统挂载点：Linux的文件系统逻辑，所有文件或目录都以“/”根目录作为起点形成树形结构，这个树形结构是将多个独立的文件系统（如ext4、tmpfs、FAT32）连接到这个树形结构的某个节点上，这个连接点就是挂载点；即解决了多种存储设备如何融入统一目录树的问题，即每一种存储设备都有对应文件系统（即存储设备的数据组织格式），通过挂载操作将该文件系统的根目录与Linux全局目录树的某个节点关联，这个节点就是对应存储设备在Linux目录树上的挂载点

Namespace不等于全局资源，而是全局资源的隔离视图，即全局资源是原始池，Namespace是对这个原始池的切片，每个切片内的进程都只能看到自己切片内的资源，看不到其他切片或原始池的资源

？？？cgroup控制组是Linux内核提供的资源限制与监控机制，而cgroup文件系统是cgroup机制的用户态操作接口，通过文件/目录的形式，让用户能直观配置cgroup规则，是伪文件系统，数据存于内核，不占磁盘空间



1.3. **采用seccomp-bpf实现*系统调用过滤***
原理是通过bpf程序自定义过滤规则，进而根据过滤规则判断是否允许进程执行该系统调用
seccomp-bpf是Linux内核提供的一种系统调用过滤机制，能够控制进程可执行的系统调用，seccomp-bpf机制，seccomp最初仅支持严格模式，即一旦启用进程只能执行read、write、exit、sigreturn（用于在用户态程序处理完信号后帮助程序恢复到信号触发前的执行上下文，让程序能够继续正常运行）四个系统调用，过于严格无法满足实际需求，后来引入了seccomp-bpf作为增强版，允许通过bpf程序自定义过滤规则，当进程启用seccomp-bpf后，所有系统调用首先会被内核捕获并将系统调用信息传递给预加载的bpf过滤程序，由bpf过滤程序判断是否允许该系统调用
bpf原本是为网络数据包过滤设计的轻量级虚拟机技术，可以将bpf理解为运行在内核中的微型变成语言，用户编写bpf程序，加载到内核后，内核会在特定时间时执行该程序，实现自定义逻辑

oj系统要支持C、C++、java、python、go、JavaScript等编程语言，可以设置以下系统调用白名单
基础I/O与文件操作：read、write、close、open、openat（相对路径打开文件）、pread64（指定偏移量读取文件）、pwrite64（指定偏移量写入文件）
内存管理：mmp（内存映射文件/匿名内存）、munmap（接触内存映射）、brk（调整进程堆空间）、mprotect（设置内存页保护）、mlock（锁定内存）
进程与信号量处理：exit_group（终止所有进程/线程）、rt_sigaction（设置信号处理函数）、rt_sigprocmask（屏蔽/接触信号屏蔽）、rt_sigreturn（信号处理后恢复上下文）
线程与同步：futex（用户态同步原语）、nanosleep（高精度休眠）、set_tid_address（设置线程id地址）
动态链接与运行时加载：access（检查文件访问权限）、readlink（读取符号链接目标）、stat（获取文件状态）、fstat（通过文件描述符获取文件状态）
架构与运行时支持：arch_prctl（设置架构相关寄存器）、uname（获取系统信息）



2. **资源分配问题，严格限制判题进程资源使用上限，避免用户代码死循环、内存泄漏等耗尽系统资源**

2.1. **采用setrlimit实现基础资源限制**
setrlimit是Linux内核实现的POSIX标准接口（Unix可移植操作系统接口），用于为进程及其子进程设置资源使用上限，底层原理是通过内核的资源计数器+信号触发机制实现。内核会为进程维护资源描述符，其中记录了各类资源的软限制和硬限制，进程通过setrlimit系统调用向内核传递资源类型和限制值，内核更新该进程的资源描述符结构体，随后内核在进程执行过程中实时跟踪资源使用情况，当资源使用超过限制时，内核通过信号机制进行干预；但setrlimit对内存的限制不精确，比如，限制的是虚拟内存而不是物理内存+swap，并且CPU限制不区分多核，并且无法限制I/O等待时间

2.2. **采用cgroups实现精确资源控制**
cgroups是Linux内核提供的模块化资源管理机制，能实现更精确、更细粒度、跨进程的资源限制，其核心思想是将进程划分为组，也就是cgroup，为每个组配置资源的配额上限，由内核的子系统实时监控并强制执行限制，子系统是内核中负责特定资源管控的模块（如内存子系统、CPU子系统），通过挂钩内核关键路径实现资源监控与限制（即在内核处理资源操作的路径上嵌入子系统的控制逻辑，从而实现对资源的实时监控和强制限制）。cgroups不提供命令行工具，其核心接口是sysfs伪文件系统（挂载在/sys/fs/cgroup/目录），每个cgroup对应sysfs伪文件系统中的一个目录，目录内的文件就是该组的资源配置项（所有配置以文件形式存在）

cgroups更加精确，源于以下三个机制，实现了内核级的实时监控与强制限制
①资源限制：通过修改文件为组配置硬上限，超限时内核直接干预
②资源记账：通过实时统计组内资源使用量，提供精确数据
③优先级调度：可以为不同权重的分组组分配资源使用优先级

cgroups解决了setrlimit资源限制的短板
①setrlimit的RLIMIT_AS限制的是虚拟内存，存在误判情况，即用户通过mmap申请大量虚拟内存但不实际写入也会触发限制；而cgroups的memory子系统直接限制物理内存+交换分区的总使用量，实现精准的内存使用量限制
②setrlimit的RLIMIT_CPU限制的是CPU计算时间，不包含I/O等待时间，在多核场景下若用户代码创建多线程，RLIMIT_CPU会累计所有核心的计算时间导致实际运行时间远超预期；并且无法限制CPU使用率，若用户代码死循环占用100%单核CPU，会导致其他判题任务卡顿；cgroups的CPU子系统通过周期+配额机制解决了这些问题，能够精确控制CPU使用率，并支持多核调度隔离，还区分计算时间和等待时间，结合墙上时钟可以精准判断CPU超时还是I/O等待超时
③setrlimit仅支持单进程限制，而cgroups支持组级别管控，即子进程自动继承父进程的cgroup归属，并且整个用户代码进程树的资源使用会被合并统计，避免子进程单独占用资源

既然setrlimit存在那么多短板，为什么不直接用cgroups实现资源限制
setrlimit缺点：限制虚拟内存存在误判并无法限制物理内存和共享内存、CPU限制仅针对计算时间不覆盖I/O等待时间、多核场景下累加所有核心计算时间远超实际代码运行时间
但是setrlimit更加轻量级，仅需要在fork子进程后调用一次syscall.Setrlimit并设置几个资源参数即可，几乎无额外的内核开销，因为本质是给进程内核结构体资源字段赋值，后续直接由内核进行逻辑检查；而cgroups比较重，会为每个判题任务动态创建控制组目录、写入资源配置文件、绑定进程PID到控制组、判题结束还要删除控制组目录，每个任务都会产生额外的内核开销；另外一方面，setrlimit是POSIX标准接口，支持Linux、BSD、macOS等系统，而cgroups仅支持Linux；setrlimit能覆盖cgroups无法触及的进程内部资源，如setrlimit能限制进程栈最大尺寸防止递归溢出，cgroups只能通过总内存限制简介控制、setrlimit能限制当前用户id的最大进程数，而cgroups只能控制组内进程总数、setrlimit能限制单个文件最大尺寸防止写爆磁盘，而cgroups只能限制磁盘IO总量、setrlimit禁用核心转储文件，而cgroups无相关配置

为什么只选择cgraoups，或者setrlimit和cgroups如何实现配合使用？实现细节上如何设计

一个判题任务要归入哪个cgroup组，更加具体的设计是怎样的

runC容器运行时的底层原理是怎样的，从源码角度进行分析

分析父进程墙上时钟兜底监控原理


2.3. **父进程墙上时钟兜底监控**

