# 高并发设计

1. 前端层：基于JavaScript监听“提交”按钮点击事件，限制提交频率

2. 接入层：在k8s集群外部署nginx作为公网入口，nginx负载均衡选择k8s集群的提交服务service（**高**）；k8s内置kube-proxy进行负载均衡，将请求分发给提交服务pod进行处理（**高**）

3. 服务层：前端限制提交频率易被绕过，需要加入redis用户级限流、全局限流兜底策略；基于ants库创建大小固定的协程池，所有请求的处理逻辑都交给协程池执行，请求量超过协程池容量则放入带缓冲的channel（任务队列）中，队列满则返回错误（**高**）

4. 任务缓冲层：kafka队列设计，削峰填谷，解耦提交服务和判题服务，按“题目ID哈希”分区（确保同一题目的提交任务分配到同一分区），多少个判题服务对应多少个分区（每个实例消费一个分区，避免任务重复处理）（**高**）


不应该直接把实现限制在某一个问题中，应该把整个流程的全部实现梳理出来，因为什么要这样做，这样做引入了什么问题又是怎么解决，最后再根据具体的问题，从实现中拿出来做回答

1. 为了防止用户恶意刷题，在前端层面基于JavaScript监听“提交”按钮点击事件，限制提交频率，而前端容易被绕过，在服务层提供分布式限流机制作为兜底，实现用户级限流与全局限流，通过redis缓存，创建用户提交次数记录和全局提交次数记录，当超过限定阈值时拒绝用户的提交请求

2. 为了避免单点过载问题，提高系统的负载能力，设计了双层负载均衡策略，通过nginx作为边缘层对后端的提交服务service做负载均衡，请求来到提交服务service后，通过k8s再对后端的提交服务pod做负载均衡

3. 为了避免无限制创建goroutine，采用ants库实现协程池，请求优先调用协程池中的协程进行处理，超过协程池容量则放入一个带缓冲的channel中，队列满则返回错误

4. 为了避免判题服务被瞬时流量压垮，引入消息队列削峰填谷，异步处理判题任务，按“题目ID哈希”分区确保同一题目的提交任务分配到同一分区，每个判题服务实例消费一个分区，避免任务重复处理

5. 为了提升用户体验，提交任务校验通过后立即生成一个提交id并返回用户，采用批量异步的方式写入数据库，减少数据库io次数，并将用户状态、题目状态等热点数据存入缓存，减少rpc调用次数


最最有优先的是底层判题任务如何执行

1. **代码安全问题，防止用户代码攻击系统**

1.1. **采用chroot实现*文件隔离***
原理是修改进程的根目录指针，从而限制进程只能访问指定目录内的文件，底层原理是Linux内核中，每个进程的进程描述符都包含一个fs字段指向fs_struct结构体，其中root成员是一个指向进程当前的根目录的指针，当调用sys_chroot时，内核会解析传入的目录路径，验证调用者是否有该目录的执行权限，再将root指针更新为该目录，后续该进程的所有路径解析都会以新的root作为起点，如调用sys_chroot指令传入目录/judger/run/123，后续进程访问/etc/passwd实际访问的是/judger/run/123/etc/passwd，若这个文件不存在则会访问失败，从而实现了文件隔离
在判题系统实现中，我们仅复制运行程序必须的文件到chroot目录中，从而最小化判题文件环境，此外还通过权限控制防止chroot逃逸（即若进程有root权限且能chdir（与cd命令功能相同，用于改变当前所在的工作目录）到上层目录，则可再次chroot突破）的发生
以上实现能阻止用户代码访问敏感文件，从文件系统层面切断攻击路径

实现方式：

1.2. **采用Linux Namespaces实现*资源隔离***
原理是通过为进程创建独立的资源视图，Linux Namespaces是Linux内核提供的一种全局资源隔离机制，其核心是通过为进程创建独立的资源视图，使得不同Namespaces中的进程仿佛运行在完全独立的系统环境中，无法感知到其他Namespace的资源，这种隔离也是容器技术Docker、Kubernetes的底层基础。Linux内核中每个进程的进程描述符都有一个nsproxy指针指向该进程所属的所有Namespace，并且新进程默认继承父进程的Namespace，但是父进程可以在创建子进程时将子进程的nsproxy指向全新的Namespace集合，当进程访问全局资源时，内核会先检查进程所属的Namespace，返回该Namespace内的资源视图，而不是全局资源

Linux七种Namespaces对应的全局资源
创建新Namespace时创建新资源，并由内核维护映射关系：
 - *PID Namespace*：管理进程id、进程组id、会话id，不同Namespace中的进程id独立，且看不到其他Namespace中的进程；内核维护“进程id映射表”，所有进程默认共享这个pid编号，通过创建新的PID Namespace可以创建独立的PID映射表，此外内核会维护不同Namespace间的pid映射关系；判题系统中可以用来防止用户代码通过kill终止主机进程，或通过ps窥探主机进程列表
 - *Cgroup Namespace*：管理控制组的视图和权限，每个Namespace仅能看到自身关联的cgroup子树；内核的cgroup文件系统是全局资源，默认进程可以看到所有cgroup子树，创建新Cgroup Namespace时，内核会对其Cgroup映射到主机Cgroup的某个子目录，如新cgroup Namespace的cgroup（/sys/fs/cgroup/）其实对应主机的/sys/fs/cgroup/judge/123，无法访问主机其他cgroup目录；判题系统中用来防止用户代码修改cgroup配置

创建新Namespace时复制父Namespace对应的资源，新Namespace中修改不影响父Namespace资源
 - *Mount Namespace*：管理文件系统挂载点，每个Namespace中文件系统挂载操作仅对自身Namespace的挂载点生效，不影响其他主机或Namespace的挂载点；内核挂载点列表是全局资源，默认所有进程看到的文件系统结构都是一样的，通过Mount Namespace创建新的Namespace时，会复制当前Namespace的挂载点列表，但是Namespace内的操作不会影响父进程；判题系统中可以用来防止用户通过mount挂在主机敏感分区，或umount破坏主机文件系统
 - *User Namespace*：管理用户id、组id、权限能力，每个Namespace有独立的uid/gid映射；内核的uid/gid权限是全局资源，默认进程的uid/gid直接对应主机的用户身份，创建新User Namespace可以进行uid/gid映射修改，即将沙箱内的root权限uid映射到主机中普通用户的uid，从而沙箱中无法执行主机级别的特权操作；判题系统中可以用来限制权限滥用，即获取沙箱中的root也无法获取主机的真实特权
 - *UTS Namespace*：管理主机名、域名，每个Namespace可以设置独立的主机名/域名，进程获取的主机信息仅为所属Namespace的配置；内核的系统标识是全局资源，默认所有进程获取的都是主机的真实主机名，创建新UTS Namespace会复制当前Namespace的主机名/域名作为初始值，后续修改仅对自身生效；判题系统中可以用来避免用户代码修改主机名干扰系统标识，或通过主机名判断系统环境

创建新Namespace时创建独立新资源
 - *Network Namespace*：管理网络设备如网卡、lo，IP地址、端口、路由表、防火墙规则、套接字，每个Namespace都有独立的网络栈，进程无法访问其他Namespace的网络资源；内核维护的网络子系统是全局资源，所有进程默认共享主机的网络栈，但可以通过Newtork Namespace为新Namespace创建一套最小化网络栈；判题系统中可以用来禁止代码的网络访问，防止恶意代码发起网络攻击或泄漏主机信息
 - *IPC Namespace*：管理进程间通信资源，如消息队列、共享内存、信号量，每个Namespace内的IPC资源仅对自身可见，进程无法通过IPC与其他Namespace的进程通信；内核的IPC资源池是全局资源，默认不同进程可以通过相同的IPC访问同一资源，创建新IPC Namespace时会分配独立的IPC资源池，仅在当前Namespace中可见；判题系统中可以用来防止用户代码通过共享内存读取主机进程的敏感数据，或通过消息队列与其他进程通信

文件系统挂载点：Linux的文件系统逻辑，所有文件或目录都以“/”根目录作为起点形成树形结构，这个树形结构是将多个独立的文件系统（如ext4、tmpfs、FAT32）连接到这个树形结构的某个节点上，这个连接点就是挂载点；即解决了多种存储设备如何融入统一目录树的问题，即每一种存储设备都有对应文件系统（即存储设备的数据组织格式），通过挂载操作将该文件系统的根目录与Linux全局目录树的某个节点关联，这个节点就是对应存储设备在Linux目录树上的挂载点

Namespace不等于全局资源，而是全局资源的隔离视图，即全局资源是原始池，Namespace是对这个原始池的切片，每个切片内的进程都只能看到自己切片内的资源，看不到其他切片或原始池的资源

1.3. **采用seccomp-bpf实现*系统调用过滤***
原理是通过bpf程序自定义过滤规则，进而根据过滤规则判断是否允许进程执行该系统调用
seccomp-bpf是Linux内核提供的一种系统调用过滤机制，能够控制进程可执行的系统调用，seccomp-bpf机制，seccomp最初仅支持严格模式，即一旦启用进程只能执行read、write、exit、sigreturn（用于在用户态程序处理完信号后帮助程序恢复到信号触发前的执行上下文，让程序能够继续正常运行）四个系统调用，过于严格无法满足实际需求，后来引入了seccomp-bpf作为增强版，允许通过bpf程序自定义过滤规则，当进程启用seccomp-bpf后，所有系统调用首先会被内核捕获并将系统调用信息传递给预加载的bpf过滤程序，由bpf过滤程序判断是否允许该系统调用
bpf原本是为网络数据包过滤设计的轻量级虚拟机技术，可以将bpf理解为运行在内核中的微型变成语言，用户编写bpf程序，加载到内核后，内核会在特定时间时执行该程序，实现自定义逻辑

oj系统要支持C、C++、java、python、go、JavaScript等编程语言，可以设置以下系统调用白名单
基础I/O与文件操作：read、write、close、open、openat（相对路径打开文件）、pread64（指定偏移量读取文件）、pwrite64（指定偏移量写入文件）
内存管理：mmp（内存映射文件/匿名内存）、munmap（接触内存映射）、brk（调整进程堆空间）、mprotect（设置内存页保护）、mlock（锁定内存）
进程与信号量处理：exit_group（终止所有进程/线程）、rt_sigaction（设置信号处理函数）、rt_sigprocmask（屏蔽/接触信号屏蔽）、rt_sigreturn（信号处理后恢复上下文）
线程与同步：futex（用户态同步原语）、nanosleep（高精度休眠）、set_tid_address（设置线程id地址）
动态链接与运行时加载：access（检查文件访问权限）、readlink（读取符号链接目标）、stat（获取文件状态）、fstat（通过文件描述符获取文件状态）
架构与运行时支持：arch_prctl（设置架构相关寄存器）、uname（获取系统信息）



2. **资源分配问题，严格限制判题进程资源使用上限，避免用户代码死循环、内存泄漏等耗尽系统资源**

2.1. **采用setrlimit实现基础资源限制**
setrlimit是Linux内核实现的POSIX标准接口（Unix可移植操作系统接口），用于为进程及其子进程设置资源使用上限，底层原理是通过内核的资源计数器+信号触发机制实现。内核会为进程维护资源描述符，其中记录了各类资源的软限制和硬限制，进程通过setrlimit系统调用向内核传递资源类型和限制值，内核更新该进程的资源描述符结构体，随后内核在进程执行过程中实时跟踪资源使用情况，当资源使用超过限制时，内核通过信号机制进行干预；但setrlimit对内存的限制不精确，比如，限制的是虚拟内存而不是物理内存+swap，并且CPU限制不区分多核，并且无法限制I/O等待时间

2.2. **采用cgroups实现精确资源控制**
cgroups是Linux内核提供的模块化资源管理机制，能实现更精确、更细粒度、跨进程的资源限制，其核心思想是将进程划分为组，也就是cgroup，为每个组配置资源的配额上限，由内核的子系统实时监控并强制执行限制，子系统是内核中负责特定资源管控的模块（如内存子系统、CPU子系统），通过挂钩内核关键路径实现资源监控与限制（即在内核处理资源操作的路径上嵌入子系统的控制逻辑，从而实现对资源的实时监控和强制限制）。cgroups不提供命令行工具，其核心接口是sysfs伪文件系统（挂载在/sys/fs/cgroup/目录），每个cgroup对应sysfs伪文件系统中的一个目录，目录内的文件就是该组的资源配置项（所有配置以文件形式存在）

cgroups更加精确，源于以下三个机制，实现了内核级的实时监控与强制限制
①资源限制：通过修改文件为组配置硬上限，超限时内核直接干预
②资源记账：通过实时统计组内资源使用量，提供精确数据
③优先级调度：可以为不同权重的分组组分配资源使用优先级

cgroups解决了setrlimit资源限制的短板
①setrlimit的RLIMIT_AS限制的是虚拟内存，存在误判情况，即用户通过mmap申请大量虚拟内存但不实际写入也会触发限制；而cgroups的memory子系统直接限制物理内存+交换分区的总使用量，实现精准的内存使用量限制
②setrlimit的RLIMIT_CPU限制的是CPU计算时间，不包含I/O等待时间，在多核场景下若用户代码创建多线程，RLIMIT_CPU会累计所有核心的计算时间导致实际运行时间远超预期；并且无法限制CPU使用率，若用户代码死循环占用100%单核CPU，会导致其他判题任务卡顿；cgroups的CPU子系统通过周期+配额机制解决了这些问题，能够精确控制CPU使用率，并支持多核调度隔离，还区分计算时间和等待时间，结合墙上时钟可以精准判断CPU超时还是I/O等待超时
③setrlimit仅支持单进程限制，而cgroups支持组级别管控，即子进程自动继承父进程的cgroup归属，并且整个用户代码进程树的资源使用会被合并统计，避免子进程单独占用资源

既然setrlimit存在那么多短板，为什么不直接用cgroups实现资源限制
setrlimit缺点：限制虚拟内存存在误判并无法限制物理内存和共享内存、CPU限制仅针对计算时间不覆盖I/O等待时间、多核场景下累加所有核心计算时间远超实际代码运行时间
但是setrlimit更加轻量级，仅需要在fork子进程后调用一次syscall.Setrlimit并设置几个资源参数即可，几乎无额外的内核开销，因为本质是给进程内核结构体资源字段赋值，后续直接由内核进行逻辑检查；而cgroups比较重，会为每个判题任务动态创建控制组目录、写入资源配置文件、绑定进程PID到控制组、判题结束还要删除控制组目录，每个任务都会产生额外的内核开销；另外一方面，setrlimit是POSIX标准接口，支持Linux、BSD、macOS等系统，而cgroups仅支持Linux；setrlimit能覆盖cgroups无法触及的进程内部资源，如setrlimit能限制进程栈最大尺寸防止递归溢出，cgroups只能通过总内存限制简介控制、setrlimit能限制当前用户id的最大进程数，而cgroups只能控制组内进程总数、setrlimit能限制单个文件最大尺寸防止写爆磁盘，而cgroups只能限制磁盘IO总量、setrlimit禁用核心转储文件，而cgroups无相关配置




2.3. **父进程墙上时钟兜底监控**

本质是以系统真实时间为基准，为用户代码进程设定一个最大存活时间，一旦任务的实际执行时间超过这个时长就强制终止，墙上时钟监控必须由与目标进程完全独立的外部进程负责监控，父进程在fork创建子进程的瞬间，记录此时系统时间，同时启动一个超时定时器，设置最大允许运行时长，子进程正常结束则取消定时器，若定时器先被触发则判定任务超时直接kill系统调用向子进程发送sigkill信号强制终止子进程
墙上时钟通过计算真实世界时间，能够解决非CPU密集型超时的监控盲区，即用户代码长时间存货却不消耗CPU，如I/O阻塞、无限睡眠、死锁等场景；同时还能保障判题流程的时间公平性，与setrlimit、cgroups一起构成三层资源防护体系，三者互补




为什么只选择cgraoups，或者setrlimit和cgroups如何实现配合使用？实现细节上如何设计

一个判题任务要归入哪个cgroup组，更加具体的设计是怎样的

runC容器运行时的底层原理是怎样的，从源码角度进行分析


    "username": "testuser",
    "password": "Test123456"



开发问题
1. kafka连接问题，网络配置，kafka容器内通过9092通信，映射到容器外是9094访问，从发现手动验证（直接通过容器内部命令验证）能够成功推送到kafka从而发现，是潜在网络问题
2. **提交服务要配有kafka消费者来消费判题结果**（这里结果如何同步给用户还不是很理解）
3. 安全配置环境下，运行c++代码报错一些问题，比如seccomp程序初始化报错内存不足（即go运行时在启动seccomp程序时无法分配内存页）（即原先是通过go语言编译一个seccomp放到工作目录中，在沙箱中运行这个seccomp程序，但是go运行时在启动这个seccomp程序时内存页分配失败），测试了减少内存优化go程序的编译，还是不行，通过在初始化程序中内嵌C语言版本的seccomp初始化程序来解决（即创建一个C语言的程序并编译放在工作目录中运行从而实现对系统调用的限制，最后通过使用C语言的libseccomp库来启动，但后面还是报错runtime_error，通过放开seccomp然后记录程序使用的系统调用，补充未满足的系统调用）（通过dmesg | grep "audit.*type=1326" | tail -5查看seccomp审计记录，查看被阻止的系统调用，从而为程序的运行补充必要的系统调用），原理是C语言启动内存较少，启动速度更快（最后通过strace跟踪系统调用从而针对性开放系统调用权限）（实际上java和python还是一开启seccomp就runtime_error，运行不了，暂时不考虑这个问题了，C语言能行就可以）
4. **kafka要异步、测试用例支持上传，并调用题目服务获取而不是直接写死**，判题服务通过内部接口调用获取测试用例和题目，不需要令牌验证（内部接口通过内部认证中间件，进行api密钥认证，即内部接口请求需要有一个头部字段包含正确的api密钥，此外内部接口用户也抓包不到，除非已经控制了服务器才能抓到对应的包、IP白名单限制，即限制本地访+docker网络访问）
5. docker容器和微服务都在一台机子上启动，判题服务容易崩溃停止，运行内存不足了，可以将一些不必要的docker容器停止，或者将docker容器迁移到另一个服务器，判题服务经常崩溃，从kafka容器日志可以发现原因是kafka消费者配置缺少心跳和会话超时参数，导致判题服务的kafka消费者心跳超时被kafka服务器踢出消费者组，服务不断重新加入消费组，服务不断尝试重新加入消费组但失败，通过添加心跳配置来解决连接稳定性问题，消费组每3秒向kafka发送心跳证明自己还活着，kafka服务器30秒内没收到心跳才踢出消费者，并且设置读取超时时间，避免无限期阻塞，通过上下文取消优雅中断并自动重试，不退出消费循环，根本原因在于没有定时向kafka发送心跳
6. **目前服务间通信采用直接调用方式，未实现consul服务治理**
7. 需要通过websocket结合异步机制实现判题结果实时回传，这里异步机制是什么意思，如何实现判题结果实时回传：即提交服务也需要作为消费者消费判题结果主题的消息，这样提交服务才能获得判题结果，然后使用websocket机制主动推送给前端用户，需要kafka配合，这里异步指的是用户提交代码后立即响应不等待判题完成，通过kafka实现服务间的异步通信，避免服务直接阻塞调用，通过websocket异步特性实时推送不需要前端轮询查询结果
8. 提交服务注册到consul：当提交服务启动时，会从yaml配置文件读取consul配置，并创建consul客户端，构建服务注册信息，然后向consul发送服务注册请求，注册成功后，consul会根据配置定期进行健康检查提交服务的状态；当其他服务需要调用提交服务的api时，调用consul客户端查询所有健康的提交服务实例，并根据负载均衡策略选择一个实例进行rpc调用，配置驱动的设计，即通过配置控制行为，即配置可以设置是否启用服务的consul功能；采用了双重服务调用策略，fallback机制，当服务发现失败或rpc调用失败时触发，通过http协议直接调用固定配置的判题服务地址直接发起http请求；rpc调用虽然也是http协议，但是rpc调用通过服务发现获取实例列表，根据负载均衡策略选择实例，然后http调用选中的实例发送请求；虽然两者都是http协议，但是rpc的实现带有服务发现、负载均衡等微服务治理功能，fallback只是简单的点对点的http调用
5. **k8s部署，双层负载均衡设置**
6. **判题接口压测**

目前c/c++就能在seccomp限制下正常运行程序，暂时关闭了java和python语言程序的seccomp限制，也能正常运行程序

strace命令是什么，为什么就能看到一个程序运行所需要的系统调用strace -c python3 -c "print('hello')" 2>&1 | head -20

跨服务的调用是不是只有判题服务调用题目服务获取题目详情和测试用例详情？：是的，仅有一个rpc调用，即判题服务调用题目服务，目前服务职责混乱，需要重构（如对题目可见性的验证直接在题目服务做，不在判题和提交服务做），重构后的业务流程逻辑如下
submission-api → 推送判题任务到Kafka → judge-api → rpc调用题目服务获取题目信息和测试用例 + 执行判题
judge-api完成判题 → 推送结果到Kafka → submission-api消费并存储 → 用户查询直接从数据库返回判题结果
用户提交代码 -> 提交服务返回submission_id，前端显示“判题中” -> 判题服务完成判题，通过kafka通知提交服务，并websocket推送给前端 -> 前端收到websocket通知则“判题完成” -> 前端发起http请求提交服务获取详细判题结果 -> 提交服务直接从数据库读取并返回完整结果

服务职责划分：
题目服务：
核心职责:
  - 题目数据管理 (CRUD)
  - 权限控制和数据过滤
  - 测试用例管理
  
权限控制策略:
  - 题目列表查询时根据用户角色过滤
  - 只返回用户有权限访问的题目
  - 比赛时间窗口控制
  - 私有题目的访问控制

API接口:
  - GET /api/v1/problems (已按用户权限过滤)
  - GET /api/v1/problems/{id} (验证访问权限)
  - POST /internal/v1/problems/{id} (内部接口，供判题服务使用)
  - GET /internal/v1/problems/{id}/test-cases (内部接口)

提交服务：
核心职责:
  - 接收用户提交请求
  - 基本参数验证
  - 推送判题任务到Kafka
  - 消费判题结果并推送WebSocket
  - 提供基于数据库的结果查询

简化职责:
  - ❌ 不再调用题目服务验证权限
  - ❌ 不再调用判题服务查询结果
  - ✅ 只做基本验证和数据传递

API接口:
  - POST /api/v1/submissions (接收提交)
  - GET /api/v1/submissions/{id}/result (从数据库查询)
  - GET /api/v1/submissions/{id}/status (从数据库查询)
  - WebSocket: /ws/submissions/{id}/status (实时推送)

判题服务：
核心职责:
  - 消费Kafka判题任务
  - 调用题目服务获取题目详情和测试用例
  - 执行判题逻辑
  - 推送判题结果到Kafka

增强职责:
  - ✅ 承担题目权限验证职责
  - ✅ 获取完整题目信息和测试用例
  - ✅ 确保判题的安全性和正确性

内部逻辑:
  - 从Kafka消费判题任务
  - 通过RPC调用题目服务获取题目信息
  - 执行编译和运行
  - 推送结果到Kafka

重构后的整个判题流程：用户通过题目服务获取题目，题目服务进行权限验证后仅返回公开可见的题目给到用户展示，用户针对一个题目进行解答，解答完毕后提交，提交会调用提交服务的接口，提交服务会做认证等参数校验后，将判题任务推入到kafka中并直接返回用户响应显示“判题中”，判题服务通过监听kafka判题任务主题，并进行消费，判题服务rpc调用题目服务获取题目详细信息和测试用例，执行具体的判题任务（fork子进程运行解答程序，通过chroot实现文件隔离，Namespaces实现资源隔离，seccomp实现系统调用过滤，setrlimit+cgroup实现资源控制），并将判题结果推入kafka，提交服务作为消费者监听判题结果主题，监听到对应判题结果后，通过websocket通知用户判题结果，前端调用提交服务的判题结果查询接口查询到判题结果详细信息并展示给用户，至此完成整个流程

用户 → 题目服务(权限验证) → 可见题目
  ↓
用户解答 → 提交服务(认证+校验) → Kafka判题任务 → 返回"判题中"
  ↓
判题服务(Kafka消费) → RPC调用题目服务 → 获取题目详情+测试用例
  ↓
执行判题(沙箱隔离) → Kafka判题结果
  ↓
提交服务(消费结果) → WebSocket推送 → 用户查询详情

正在实现：websocket实时推送、实现consul服务治理与rpc服务间调用，测试重构后功能是否正常，接下来等待测试正常后，进行判题接口压测，再实现k8s部署，实现两层负载均衡，再进行压测


待做：
1. 判题结果消费者是在哪个服务实现的
2. 提交服务提交判题任务，判题服务执行判题任务并返回结果，这中间整个过程数据库层面会保存哪些信息？
3. consul服务注册与发现和rpc调用之间有什么关系，如何实现rpc调用的
4. 通过websocket实时推送的原理是什么，websocket原理，和长轮询的区别
5. 判题状态是如何获取的
6. 要继续到底层了解实际判题的时候，安全措施这些如何生效的，到底有没有fork子进程来执行判题任务，以及任务队列是如何实现